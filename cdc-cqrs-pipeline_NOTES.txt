Debezium & Kafka Change Data Capture
https://www.youtube.com/watch?v=WPN68WrpVJs

Note: Debezium is a Source Connector and NOT a Sink Connector

 How to do CDC using debezium, kafka and postgres
https://www.youtube.com/watch?v=qXIAsgldOB8
https://www.startdataengineering.com/post/change-data-capture-using-debezium-kafka-and-pg/

 

 MongoDB Connector
Run Confluent / Confluent Kafka using docker and install connector
https://www.youtube.com/watch?v=qHxm7Eu3C5Y

 
 
 Step 1. Run MySQL via docker-compose.yml
 Step 2: Run SQL on it
 
 Docker-Compose & Mysql Links
https://dev.to/sonyarianto/how-to-spin-mysql-server-with-docker-and-docker-compose-33b2
https://medium.com/@chrischuck35/how-to-create-a-mysql-instance-with-docker-compose-1598f3cc1bee
https://geshan.com.np/blog/2022/02/mysql-docker-compose/
https://learn.microsoft.com/en-us/visualstudio/docker/tutorials/tutorial-multi-container-app-mysql
https://blog.devgenius.io/how-i-setup-mysql-in-docker-compose-e05ba7bcfece
https://www.youtube.com/watch?v=krDi5y3iHmY
https://citizix.com/how-to-run-mysql-8-with-docker-and-docker-compose/


Notes
=========================================================================================
Docker
================================
*) To start mongoDB Server, open a VS Code Terminal and go to the folder location where your docker-compose.yml exists:
	> cd D:\Work\EclipseWorkspace\Github\spring-boot-mongodb
	> docker-compose up
	This should start MongoDB Server.	
*) To remove all volumes (data will be erased) created with docker-compose up use: 
	> docker-compose down -v	


MySQL
================================
*) To access Adminer (MySQL Client) use: http://localhost:8082/
*) To access MySQL by logging into the container:
	- > docker exec -it mysql_db_server /bin/bash
	- > mysql -uuser -p
	- Enter Password: 
	- > show databases;
	- To log out type "exit;" twice
	
	
MongoDB
=================================
To start MongoDB Client run:
	> docker exec -it <container_id> mongo admin -u rootusername -p 'rootpassword'
	OR
	> docker exec -it mongodb_server mongo admin -u rootusername -p 'rootpassword'
Now you should be connected to MongoDB Client.
*) Test to check if the Mongo Client can talk to Mongo Server:
	> db.version()
*) To exit from mongo shell use:
	> exit	
*) To access Mongo-Express (UI Client for MongoDB): http://localhost:8081


Kafka
====================================
*) To access Kafka from the shell:
	> docker exec -it kafka_server bash
*) To create a topic
	> kafka-topics --bootstrap-server=localhost:9092 --topic first_topic --create
*) To list topics:	
	> kafka-topics --bootstrap-server=localhost:9092 --list
*) To exit type "exit"
*) To produce & consumer on Kafka:
	> docker run -it --rm --name kafka_client_producer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-console-producer --bootstrap-server kafka_server:29092 --topic test_topic
	> docker run -it --rm --name kafka_client_consumer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-console-consumer --bootstrap-server kafka_server:29092 --topic test_topic










Debezium is a CDC (Change Data Capture) tool built on top of Kafka Connect that can stream changes in real-time from MySQL, PostgreSQL, MongoDB, Oracle, and Microsoft SQL Server into Kafka, using Kafka Connect.
Debezium CDC Kafka records historical data changes made in the source database to Kafka logs, which can be further used in a Kafka Consumer. We can then easily capture the changes via the Kafka Consumer. In case of abrupt stopping or crashing of application or Debezium CDC Kafka, when restarted the data gets consumed to form the last recorded offset, resulting in zero data loss.
Debezium’s MySQL connector reads MySQL’s binary log to understand the changes in data sequentially. A change event for every row-level operation like insert, update and delete is produced and recorded to a separate Kafka topic.

Kafka Connect is used for streaming data between Apache Kafka and other external systems. It is used to define connectors that move large collections of data in and out of Kafka.

The Naming Convention For Kafka Topics
All the change events are stored in a separate topic per table by Debezium. If the connector was configured using the following parameters:
“database.dbname”: “database_name”,
“database.server.name”: “database_server_name”,
The change events for the cdc_test table will be stored in a Kafka topic which is of the format `<hostname>.<databasename>.<tablename>`. i,e in this case database_server_name.database_name.cdc_test.
Each table in the database becomes a separate topic in Kafka containing one partition by default:
MySQL-Database-Docker.database_name.user
MySQL-Database-Docker.database_name.address
MySQL-Database-Docker.database_name.category
We can listen to these messages using the kafka-console-consumer.sh:
$KAFKA_HOME/bin/kafka-console-consumer.sh -bootstrap-server "loclahost:9092" -topic "MySQL-Database-Docker.database_name.user" -from-beginning
After running the console consumer, insert a record in the MySQL table to see the messages in the console consumer :
mysql> INSERT INTO `users` VALUES (2, ‘John Doe’);
Note the structure of the message — we have a before and after field of the record, plus a bunch of metadata (source, op, ts_ms). Depending on what you’re using the CDC events for, you’ll want to retain some or all of this metadata.
A message is a combination of two metadata: Schema and Payload. The payload consists of 2 parts: before and after, which contains the previous and current record in case of an update operation in Kafka MySQL.

Debezium’s functionality is determined by the database it employs. For example, it reads the commit log in MySQL to figure out the nature of the transactions. However, it uses MongoDB’s native replication method in MongoDB. 

Built on top of Apache Kafka (Kafka Connect REST API), Debezium has a connector library that captures changes from a number of databases and produces events with fairly similar patterns, making it easy for applications to consume and respond to the events regardless of where the changes came from. Debezium enables detecting changes and translating them to events in a single connection. Regardless of the fact that multiple connections exist, they all create events with fairly similar formats, making it much easier for your applications to receive and respond to the events irrespective of where the changes came from.

Key Features of Debezium
Database Consistency: Debezium ensures eventual consistency by relying on the database’s transaction log to catch changes. Debezium replays the WAL (Write-Ahead Log, a transactional file) in the same way as a read replica or a database backup does. In fact, one can consider the resultant topic as a read replica of the database table.
Failure Handling: Debezium keeps track of which transaction file items it had processed earlier and stores the read offset in Kafka. Debezium will continue from where it left off in case of an event failure like poor network connectivity. As a result, you can be confident that no modifications will be missed during downtime

Debezium MySQL Connector is a Source connector that can take a snapshot of existing data and track all row-level changes in databases on a MySQL server/cluster. It reads a consistent snapshot of all databases the first time it connects to a MySQL server. In other words, the MySQL connection takes an initial consistent snapshot of each of your databases because MySQL is often set up to remove binlogs after a defined length of time. The MySQL connection reads the binlog from the snapshot’s starting point.
When the snapshot is complete, the connector reads the changes that were committed to MySQL and creates INSERT, UPDATE, and DELETE events as needed. Each table’s events are stored in their own Kafka topic, where they may be readily accessed by applications and services.
The binary log (binlog) in MySQL records all database actions in the order in which they are committed. This includes both modifications to table schemas and changes to table data. The binlog is used by MySQL for replication and recovery. 
The MySQL connector also ensures against event failure. The connector records the binlog position with each event as it reads the binlog and creates events. When a connector shuts down (for example, due to communication problems, network difficulties, or crashes), it begins reading the binlog from where it left off after being resumed. This implies if a snapshotting is not finished when the connector is stopped, a new snapshot will be started when the connector is restarted

All INSERT, UPDATE, and DELETE operations from a single table are written to a single Kafka topic using the Debezium MySQL connection. The following is the Kafka topic naming convention: serverName.databaseName.tableName






docker exec -it mysql_db_server /bin/bash
mysql -uroot -p
desc mysql.db;
SHOW GRANTS FOR user@'%';
show databases;

# UPDATE mysql.user SET Super_Priv='Y' WHERE user='user' AND host='%';
# FLUSH PRIVILEGES;
GRANT SUPER, RELOAD, PROCESS, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO user@'%';
FLUSH PRIVILEGES;


# GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT  ON *.* TO 'user' IDENTIFIED BY 'password';


docker run -it --rm --name kafka_client_consumer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-console-consumer --bootstrap-server kafka_server:29092 --topic app-mysql-server.app-mysql-db.customer --from-beginning

docker run -it --rm --name kafka_client_consumer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-topics --bootstrap-server kafka_server:29092 --list

TODO: Check if we really need "Links" section in docker-compose.yml
TODO: Try calling create-connector Postman call via docker-compose.yml




TODO:
https://www.clairvoyant.ai/blog/mysql-cdc-with-apache-kafka-and-debezium ------- DONE, does not uses docker
debezium docker compose mysql
====================================================
https://morioh.com/p/f19328d78a31  ------ DONE
https://github.com/debezium/debezium-examples/blob/main/tutorial/docker-compose-mysql.yaml ----- DONE
https://github.com/nenodias/debezium-mysql/blob/main/docker-compose.yml   ------ DONE
https://github.com/xmlking/cdc-kafka-hadoop/blob/master/cdc/debezium/register-mysql.json  ------- DONE
http://www.mastertheboss.com/jboss-frameworks/debezium/how-to-capture-data-changes-with-debezium/
https://www.conduktor.io/blog/capturing-mysql-database-changes-using-debezium-kafka-and-conduktor    ----------- DONE
https://blog.devgenius.io/change-data-capture-from-mysql-to-postgresql-using-kafka-connect-and-debezium-ae8740ef3a1d
https://hevodata.com/learn/debezium-mysql/ ------- DONE, does not uses docker
https://levelup.gitconnected.com/change-data-capture-with-debezium-kafka-and-mysql-359f7bc6b29a -------- DONE
https://github.com/mongodb/mongo-kafka/tree/master/docker
https://github.com/mongodb-university/kafka-edu/tree/main/docs-examples/mongodb-kafka-base
https://github.com/ivangfr/springboot-kafka-connect-debezium-ksqldb/blob/master/docker-compose.yml
https://github.com/ivangfr/springboot-kafka-connect-debezium-ksqldb
https://github.com/debezium/debezium-examples/tree/main/tutorial#using-mongodb


*) springboot-kafka-connect-debezium-ksqldb
https://github.com/ivangfr/springboot-kafka-connect-debezium-ksqldb	




Use Postman to Send Curl Commands: https://www.shipengine.com/docs/curl/


*) Sample Configs:
{
  "name": "sde-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "start_data_engineer",
    "database.password": "password",
    "database.dbname": "start_data_engineer",
    "database.server.name": "bankserver1",
    "table.whitelist": "bank.holding"
  }
}
The database.* configs are connection parameters for our postgres database
database.server.name is a name we assign for our database
table.whitelist is a field that informs the debezium connector to only read data changes from that table. Similarly you can whitelist or blacklist tables or schemas. By default debezium reads from all tables in a schema.
connector.class is the connector used to connect to our postgres database
name name we assign to our connector


docker-compose logs -f connect


{
    "name": "mongodb-sink",
    "config": {
        "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
        "tasks.max": 1,
        "topics": "app-mysql-server.app-mysql-db.customer",
        "mongodb.connection.uri": "mongodb://mongodb_server:27017/test?retryWrites=true",
        "mongodb.collection": "MyCollection",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "key.converter.schemas.enable": false,
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": false
    }
}

  
  connect:
    image: confluentinc/cp-kafka-connect-base:7.2.2
    build:
      context: .
      dockerfile: connect.Dockerfile
    ports:
      - "35000:35000"
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper_server
      - kafka_server
      - mongodb_server
    networks:
      - app_network
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka_server:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: connect-cluster-group      
      CONNECT_CONFIG_STORAGE_TOPIC: docker-mongodb-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-mongodb-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-mongodb-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_ZOOKEEPER_CONNECT: "zookeeper_server:2181"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_CONNECTIONS_MAX_IDLE_MS: 180000
      CONNECT_METADATA_MAX_AGE_MS: 180000
      CONNECT_AUTO_CREATE_TOPICS_ENABLE: "true"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"    




  connect:
    build: docker/kafka-connect
    container_name: connect
    restart: unless-stopped
    depends_on:
      - zookeeper_server
      - kafka_server
      - mongodb_server
    networks:
      - app_network
    ports:
      - "8084:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka_server:29092
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_PARTITIONS: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_PARTITIONS: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter" 
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_PLUGIN_PATH: "/usr/share/java"
    healthcheck:
      test: "curl -f http://localhost:$$CONNECT_REST_PORT || exit 1"   



https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.9.5.Final/debezium-connector-mysql-1.9.5.Final-plugin.tar.gz


To look at logs for a particular service: docker-compose logs -f <service-name>

docker-compose logs -f kafka_connect


kafka_connect:
    #image: debezium/connect:1.6
    build: docker/kafka-connect
    hostname: kafka_connect
    container_name: kafka_connect
    networks:
      - app_network
    restart: always
    depends_on:
      - kafka_server
      - mysql_db_server
      - mongodb_server
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka_server:29092
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter" 
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka_connect
      CONNECT_PLUGIN_PATH: "/usr/share/java"
      CONNECT_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - '8083:8083'
    links:
      - zookeeper_server
      - kafka_server
      - mysql_db_server
	  




mongodb://rootusername:rootpassword@mongodb_server:27017/mydb [right]
mongodb://mongodb_server:27017	  



# Start MySQL connector
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @register-mysql.json
# Start MongoDB connector
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @register-mongodb.json



order-read-service
	- Exposes a REST endpoint to provide order-details
	- Reads the data from MongoDB
order-write-service
	- Exposes a REST endpoint to ingest order_id & shipping_details
	- Exposes a REST endpoint to ingest order_id & item_details
	- If the user provides an order_id, that will be used, otherwise something will be created
	- Persists to MySQL DB
	- CDC from MySQL DB to Kafka Topics
order-process-service
	- Read from both the topics
	- Store them in state-store
	- combine them into one Aggregate
	- push it to aggregate topic
	- data is persisted from this Aggregate topic to MongoDB via Connector
	
	
Order_Id  | Shipping_Details
Order_Id   | Item_Details	

OrderAggregate {
	Order_id
	Shipping_Details
	Item_Details	
}	





16th Jan, 2023
======================================================================================================
Steps to execute:
1. From Docker, execute "docker-compose up"

2. To access Adminer (MySQL Client) use: http://localhost:8082/
	- username=custom_mysql_user
	- password=custom_mysql_user_password
	- db=app-mysql-db

3. To access Mongo-Express (UI Client for MongoDB): http://localhost:8081

4. Execute Postman Request "Create-MySQL-Debezium-Connector"

5. Execute Postman Request "Create-MongoDB-Sink-Connector"

6. Start the Spring Boot App "order-write-service"

7. Execute Postman Request "Post-Shipping-Details"



From Docker, execute "docker-compose down -v"

docker run -it --rm --name kafka_client_consumer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-topics --bootstrap-server kafka_server:29092 --list

docker run -it --rm --name kafka_client_consumer --network app_network confluentinc/cp-kafka:7.0.1 /bin/kafka-console-consumer --bootstrap-server kafka_server:29092 --topic app-mysql-server.app-mysql-db.shipping_details




*) Udemy - Python
*) Go thru Shravan's session
*) CDC-CQRS Pipeline
*) Migrate kafka-json-registry to Github
 
*) #1. M.Tech. Data Science & Engineering
https://bits-pilani-wilp.ac.in/m-tech/cluster/data-science-and-engineering.php

*) #2. Master of Science in Data Science
https://www.upgrad.com/data-science-masters-degree-ljmu/?utm_source=GOOGLE&utm_medium=NBSEARCH&utm_campaign=IND_Acq_Web_Google_NBSearch_DV_IIITBLJMU_MDS_HIT_ROI&utm_content=MS_DS_Broad&utm_term=masters%20in%20data%20science&gclid=CjwKCAiAh9qdBhAOEiwAvxIok5NKl694XKy4OE-1oT-M7aYhO_eIL2fF2oH0_XpmlknEEfp5bTyMnRoClP0QAvD_BwE




 
Reference:
*) Analysing Changes with Debezium and Kafka Streams
https://www.confluent.io/blog/cdc-and-streaming-analytics-using-debezium-kafka/?utm_medium=sem&utm_source=google&utm_campaign=ch.sem_br.nonbrand_tp.prs_tgt.dsa_mt.dsa_rgn.india_lng.eng_dv.all_con.blog&utm_term=&creative=&device=c&placement=&gclid=CjwKCAiA5Y6eBhAbEiwA_2ZWIcWEc72lZMx8l5N0SvHXeLt98rqyQjOPXkiLa_4k0we2Qh-sSPu2kxoCNMAQAvD_BwE
*)
Text to find: Before registering the connector, you should be familiar with its configuration. In the next step, you will register the following connector:
https://debezium.io/documentation/reference/stable/tutorial.html
*)
Text to find: Here are the details of the value of the last event (formatted for readability):
https://debezium.io/documentation/reference/stable/tutorial.html
*) Creating DDD aggregates with Debezium and Kafka Streams
https://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/
Serde: https://github.com/debezium/debezium-examples/blob/main/kstreams/poc-ddd-aggregates/src/main/java/io/debezium/examples/aggregation/serdes/JsonHybridDeserializer.java
*) Introduction to Debezium
https://www.baeldung.com/debezium-intro
*) Find more about SourceRecord of Debezium
- https://github.com/zzp15222522401/daima/blob/df84b0fa1d0fb3eb5f7e69aa02320494daf90172/flinkgmall1116-parent/flinkgmall1116-realtime/src/main/java/com/atguigu/flinkgmall/funs/MyDebeziumDeserializationSchema.java
- https://github.com/sunjiale123456/Project/blob/46afcc82aec1dc73c96e34ced02e2ca231be2201/src/main/java/SchemaUDF/MyDeserializationSchema.java
- https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestPostgresDebeziumSource.java
 
*) Debezium Event Deserialization
https://debezium.io/documentation/reference/stable/integrations/serdes.html
*) Debezium connector for MySQL
 
*) Change Data Capture and Kafka to break up your monolith
https://lenses.io/blog/2021/04/change-data-capture-apache-kafka-break-up-monolith/
 
 
 
 
In this post we are going to talk about a CDC-CQRS data-pipeline between MySQL (a RDBMS database) and MongoDB (a NoSQL database).
 
Here is an architecture-diagram for what we are planning to achieve.
 
<Architecture-Diagram>
 
The major components in this Architecture Diagram are:
 
*) Shipping-Details Microservice
 
 
 
To execute this application here are the steps that need to be followed:
*) docker-compose.yml
*) Verify Kafka, MySQL, MongoDB is running
*) REST API call for Debezium Connector & MongoDB Sink Connector
*) Insert ShippingDetails & OrderItems for a particular OrderId using the two write Microservices
*) Read the OrderAggregate from the ReadMicroservice from MongoDB.




Some more useful URL:

- https://stackoverflow.com/questions/67760094/how-to-combine-different-messages-to-1-output-message-with-kstreams
- https://stackoverflow.com/questions/54165524/how-to-transform-multiple-messages-into-single-message



Parsing Json (JsonNode)
https://jenkov.com/tutorials/java-json/jackson-jsonnode.html#get-jsonnode-field


Rounded Corner Table:
https://codepen.io/mlms13/pen/DWbpwo
Include Ink Free font: https://www.cdnfonts.com/ink-free.font
body {
  margin: 30px;
  font-family: 'Garamond', sans-serif;
}



ORDER_ID	CUSTOMER_NAME	CUSTOMER_ADDRESS	ZIPCODE
ed26c8b6-0431-4c95-8555-0af0cd19e2d2	John Doe	104 Rising Sun Rd	08505
4b280363-c00d-4d3d-9a9f-f772168068f4	Jane Doe	1506 Market St	60016
754c240d-0df4-4f2d-82de-b725a98cdc8f	Jennifer Johnson	3158 Hamner Ave	92860


ORDER_ID	ITEM_ID	ITEM_NAME	PRICE	QUANTITY
ed26c8b6-0431-4c95-8555-0af0cd19e2d2	I001	Laptop	500	1
ed26c8b6-0431-4c95-8555-0af0cd19e2d2	I002	Handbag	1000	2
ed26c8b6-0431-4c95-8555-0af0cd19e2d2	I003	Book	300	3
4b280363-c00d-4d3d-9a9f-f772168068f4	I004	Jacket	250	1
4b280363-c00d-4d3d-9a9f-f772168068f4	I001	Laptop	500	1
754c240d-0df4-4f2d-82de-b725a98cdc8f	I002	Handbag	500	1

LucidChart
Microsoft Visio


